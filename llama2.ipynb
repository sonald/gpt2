{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2\n",
    "\n",
    "## llama1: https://arxiv.org/pdf/2302.13971.pdf\n",
    "\n",
    "Pre-normalization [GPT3]. To improve the training stability, we normalize the input of each transformer sub-layer, instead of normalizing the output. We use the RMSNorm normalizing function, introduced by Zhang and Sennrich (2019). \n",
    "\n",
    "SwiGLU activation function [PaLM]. We replace the ReLU non-linearity by the SwiGLU activation function, introduced by Shazeer (2020) to improve the performance. We use a dimension of $\\frac{2}{3}4d$ instead of 4d as in PaLM.\n",
    "\n",
    "Rotary Embeddings [GPTNeo]. We remove the absolute positional embeddings, and instead, add rotary positional embeddings (RoPE), introduced by Su et al. (2021), at each layer of the network.\n",
    "\n",
    "## llama2 \n",
    "\n",
    "## RMSNorm\n",
    "\n",
    "https://arxiv.org/pdf/1910.07467.pdf\n",
    "\n",
    "$ \\overline{a}_i = \\frac{a_i}{RMS(a)} g_i $ \n",
    "$ RMS(a) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N a_i^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uos/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "import json\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoTokenizer, PreTrainedModel\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_utils import no_init_weights, ContextManagers\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "\n",
    "model_path = '/data/sonald/ai_models/model_weights/Llama-2-7b-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 'meta-llama/Llama-2-7b-hf',\n",
       " 'architectures': ['LlamaForCausalLM'],\n",
       " 'bos_token_id': 1,\n",
       " 'eos_token_id': 2,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 4096,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 11008,\n",
       " 'max_position_embeddings': 4096,\n",
       " 'model_type': 'llama',\n",
       " 'num_attention_heads': 32,\n",
       " 'num_hidden_layers': 32,\n",
       " 'num_key_value_heads': 32,\n",
       " 'pretraining_tp': 1,\n",
       " 'rms_norm_eps': 1e-05,\n",
       " 'rope_scaling': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'torch_dtype': 'float16',\n",
       " 'transformers_version': '4.31.0.dev0',\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 32000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(model_path + '/config.json', mode='r') as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rms_norm_eps = config['rms_norm_eps']\n",
    "        self.weight = nn.Parameter(torch.ones(config['hidden_size']))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        var = x.pow(2).mean(dim=-1, keepdim=True) + self.rms_norm_eps\n",
    "        x = x * torch.rsqrt(var)\n",
    "        a = self.weight * x.to(dtype=dtype)\n",
    "        return a\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(\n",
    "            config['hidden_size'], config['intermediate_size'], bias=False)\n",
    "        self.up_proj = nn.Linear(\n",
    "            config['hidden_size'], config['intermediate_size'], bias=False)\n",
    "        self.down_proj = nn.Linear(\n",
    "            config['intermediate_size'], config['hidden_size'], bias=False)\n",
    "        self.act = ACT2FN[config['hidden_act']]\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.act(self.gate_proj(\n",
    "            hidden_states)) * self.up_proj(hidden_states)\n",
    "        hidden_states = self.down_proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, config, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        self.seq_len = 0\n",
    "        self.d = config['hidden_size'] // config['num_attention_heads']\n",
    "        inv_freq = 1.0 / \\\n",
    "            (base ** (torch.arange(0, self.d, 2, dtype=torch.float32).to(device) / self.d))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self._update_sin_cos_cache(config['max_position_embeddings'])\n",
    "\n",
    "    def _update_sin_cos_cache(self, seq_len):\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.float32).to(\n",
    "            self.inv_freq.device)\n",
    "        rot = einsum(pos, self.inv_freq, 'i,j -> i j')  # S, D\n",
    "        theta = torch.cat([rot, rot], dim=-1)\n",
    "        self.register_buffer('sin_cached', theta.sin()[\n",
    "                             None, None, :, :], persistent=False)\n",
    "        self.register_buffer('cos_cached', theta.cos()[\n",
    "                             None, None, :, :], persistent=False)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq_len = x.shape[-2]\n",
    "        if self.seq_len < seq_len:\n",
    "            self._update_sin_cos_cache(seq_len)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[..., :seq_len, :].to(dtype=x.dtype),\n",
    "            self.sin_cached[..., :seq_len, :].to(dtype=x.dtype)\n",
    "        )\n",
    "\n",
    "\n",
    "class LlamaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(\n",
    "            config['hidden_size'], config['hidden_size'], bias=False)\n",
    "        self.k_proj = nn.Linear(\n",
    "            config['hidden_size'], config['hidden_size'], bias=False)\n",
    "        self.v_proj = nn.Linear(\n",
    "            config['hidden_size'], config['hidden_size'], bias=False)\n",
    "        self.o_proj = nn.Linear(\n",
    "            config['hidden_size'], config['hidden_size'], bias=False)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config)\n",
    "        self.num_heads = config['num_attention_heads']\n",
    "\n",
    "        D = config['max_position_embeddings']\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(D, D))[\n",
    "                             None, None, :, :], persistent=False)\n",
    "\n",
    "    def rotate_half(self, x):\n",
    "        D = x.shape[-1] // 2\n",
    "        x1 = x[..., :D]\n",
    "        x2 = x[..., D:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        q = self.q_proj(hidden_states)  # B, S, D\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "\n",
    "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
    "\n",
    "        cos, sin = self.rotary_emb(v)\n",
    "        cos = cos.to(q.device)\n",
    "        sin = sin.to(q.device)\n",
    "\n",
    "        # apply RoPE\n",
    "        q = q * cos + self.rotate_half(q) * sin\n",
    "        k = k * cos + self.rotate_half(k) * sin\n",
    "\n",
    "        q_seq_len, kv_seq_len = q.shape[-2], v.shape[-2]\n",
    "        # SDPA is the same\n",
    "        # attn_mask = self.bias[:, :, :q_seq_len,\n",
    "        #                       :kv_seq_len] == 1  # sdpa needs bool mask\n",
    "        # attn = F.scaled_dot_product_attention(\n",
    "        #     q, k, v, is_causal=True, dropout_p=0.0, attn_mask=attn_mask)\n",
    "        scores = einsum(q, k, 'b h q d, b h k d -> b h q k') / \\\n",
    "            math.sqrt(q.shape[-1])\n",
    "        scores = scores.masked_fill(\n",
    "            self.bias[:, :, :q_seq_len, :kv_seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1, dtype=torch.float32).to(v.dtype)\n",
    "        attn = attn @ v\n",
    "\n",
    "        hidden_states = rearrange(attn, 'b h s d -> b s (h d)')\n",
    "        hidden_states = self.o_proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LlamaLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaMultiHeadAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        pre_normed = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.self_attn(pre_normed) + hidden_states\n",
    "\n",
    "        post_normed = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(post_normed) + hidden_states\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config['vocab_size'], config['hidden_size'])\n",
    "        self.layers = nn.ModuleList([LlamaLayer(config)\n",
    "                                    for _ in range(config['num_hidden_layers'])])\n",
    "        self.norm = LlamaRMSNorm(config)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Llama2ForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = Llama2Model(config)\n",
    "        self.lm_head = nn.Linear(\n",
    "            config['hidden_size'], config['vocab_size'], bias=False)\n",
    "\n",
    "        if 'tie_word_embeddings' in config and config['tie_word_embeddings']:\n",
    "            self.model.embed_tokens.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        hidden_states = self.model(input_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits.float(), hidden_states\n",
    "\n",
    "    def from_pretrained(model_path: str):\n",
    "        with open(os.path.join(model_path, 'config.json'), mode='r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        match config['torch_dtype']:\n",
    "            case 'float16':\n",
    "                dtype = torch.float16\n",
    "            case 'bfloat16':\n",
    "                dtype = torch.bfloat16\n",
    "            case _:\n",
    "                dtype = torch.float32\n",
    "        torch.set_default_dtype(dtype)\n",
    "\n",
    "        init_contexts = [no_init_weights(_enable=True)]\n",
    "        with ContextManagers(init_contexts):\n",
    "            # Let's make sure we don't run the init function of buffer modules\n",
    "            model = Llama2ForCausalLM(config)\n",
    "            model.eval()\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        inited = set()\n",
    "        print('loading state dict...')\n",
    "        files = ['/pytorch_model-00001-of-00002.bin',\n",
    "                 '/pytorch_model-00002-of-00002.bin']\n",
    "        for shard in files:\n",
    "            bin_sd = torch.load(model_path + shard)\n",
    "            for key in tqdm(bin_sd.keys(), desc=shard):\n",
    "                if key in sd_keys:\n",
    "                    with torch.no_grad():\n",
    "                        sd[key].copy_(bin_sd[key])\n",
    "                        inited.add(key)\n",
    "\n",
    "        # print(set(sd_keys) ^ (inited))\n",
    "        return model\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, input_ids: torch.Tensor, do_sample=False, max_new_tokens: int = 100):\n",
    "        input_ids = input_ids[:, -\n",
    "                              self.config['max_position_embeddings']:]  # (B, S)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(input_ids)  # B, S, V\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)  # B, V\n",
    "            if do_sample:\n",
    "                next_id = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                next_id = torch.argmax(probs, dim=-1, keepdim=True)  # B, 1\n",
    "            input_ids = torch.concat((input_ids, next_id), dim=1)\n",
    "\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading state dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch_model-00001-of-00002.bin: 100%|██████████| 241/241 [00:00<00:00, 419.89it/s]\n",
      "/pytorch_model-00002-of-00002.bin: 100%|██████████| 82/82 [00:00<00:00, 519.48it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Llama2ForCausalLM.from_pretrained(model_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, padding=True, return_tensors=\"pt\")\n",
    "    output = model.generate(inputs['input_ids'].to('cuda'), max_new_tokens=max_new_tokens)\n",
    "    response = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I enjoy walking with my cute dog, reading, and watching movies.\\nI am a very friendly person and I love to help people. I am a very hard worker and I am very dedicated to my work. I am very patient and I am very good at listening to people. I am very good at communicating with people. I am very good at problem solving. I am very good at working with people. I am very good at working with children. I am very good at working with animals. I am very good']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict(model, tokenizer, 'what is the capital of USA?')\n",
    "predict(model, tokenizer, 'I enjoy walking with my cute dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "base = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer([prompt], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "    output = model.generate(**inputs, do_sample=False, temperature=1.0, max_new_tokens=max_new_tokens)\n",
    "    response = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uos/miniconda3/envs/vllm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I enjoy walking with my cute dog, reading, and watching movies.\\nI am a very friendly person and I love to help people. I am a very hard worker and I am very dedicated to my work. I am very patient and I am very good at listening to people']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict2(base, tokenizer, 'I enjoy walking with my cute dog', max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2825, -1.1631, -0.5928,  ..., -0.1378, -0.4785, -0.0170],\n",
      "         [ 0.3035, -0.7549, -0.2247,  ..., -0.3157, -0.5298, -0.3186]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[ 0.2825, -1.1631, -0.5928,  ..., -0.1378, -0.4785, -0.0170],\n",
      "         [ 0.3037, -0.7549, -0.2246,  ..., -0.3157, -0.5293, -0.3186]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,2,4096).to('cuda')\n",
    "\n",
    "c = base.model.layers[0].self_attn(x)[0]\n",
    "d = model.model.layers[0].self_attn(x)\n",
    "print(c)\n",
    "print(d)\n",
    "\n",
    "print(torch.equal(c,d))\n",
    "print(torch.allclose(c,d, atol=1e-3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
